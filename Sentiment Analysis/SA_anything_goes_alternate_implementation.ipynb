{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sentiment analysis of the Welsh language using Word2Vec as implemented in Gensim\n",
    "NOTE:  This script relies heavily on \"MODERN METHODS FOR SENTIMENT ANALYSIS\" by Michael Czerny (with modifications)\n",
    "Czerny's tutorial is available at https://www.districtdatalabs.com/modern-methods-for-sentiment-analysis\n",
    "\n",
    "I found the pre-trained Welsh word2vec encoding at https://drive.google.com/drive/folders/1iGqzFlZifSeHzPhnz3qM68a37Ul7S7Xq\n",
    "(Unfortunately, I cannot locate the page where found this link, but it was via Cardiff University: https://www.cardiff.ac.uk/)\n",
    "\n",
    "The accuracy percentage reported is from a heldback test set. I let it run for a few hours on the provided testing set, \n",
    "but it did not finish. Even on the test/train split dataset, the accuracy is poor (68%). The approach needs\n",
    "more attention than the current deadline allows. \n",
    "'''\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "def buildWordVector(text, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += welsh_w2v.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "#Do some very minor text preprocessing\n",
    "def cleanText(corpus):\n",
    "    corpus = [z.lower().replace('\\n','').split() for z in corpus]\n",
    "    return corpus\n",
    "\n",
    "# Load data.  \n",
    "word_vectors = KeyedVectors.load_word2vec_format('welsh.vec')\n",
    "\n",
    "\"\"\"\n",
    "Experimenting ...  checking relationships between words\n",
    ">>> word_vectors.most_similar('car')\n",
    "[('cerbyd', 0.8214571475982666), ('beic', 0.7958533763885498), ('gar', 0.7678565979003906), ('lori', 0.7664268612861633), \n",
    "('tryc', 0.7579614520072937), ('char', 0.7528360486030579), ('gerbyd', 0.7441043853759766), ('tractor', 0.7217353582382202), \n",
    "('trên', 0.7176547050476074), ('moped', 0.7080420851707458)]\n",
    "    According to Google translate:\n",
    "    cerbyd -> vehicle\n",
    "    beic -> bike\n",
    "    gar - > a car\n",
    "    lori -> a truck\n",
    "    tryc -> a truck\n",
    "    char -> a car\n",
    "    gerbyd -> vehicle\n",
    "    tractor -> tractor\n",
    "    trên -> train\n",
    "    moped -> moped\n",
    "\n",
    "I am not sure whether the fact that some of the Google translations include the indefinite article others do not reflects\n",
    "an aspect of the Welsh language or a Google quirk.\n",
    "\n",
    "The documentation indicates that this method preserves/discovers nontrivial grammatic relationships.\n",
    "i.e., \"ate\" - \"eat\" + \"speak\" = \"spoke\".  Unfortunately, I don't know enough about Welsh (yet!) to investigate this here.\n",
    "\"\"\"\n",
    "\n",
    "# Specifying utf8 appears to be required, as the import generated an encoding error message otherwise.\n",
    "with open('train-v2.tsv', 'r', encoding=\"utf8\") as infile:\n",
    "    tweets = infile.readlines()\n",
    "\n",
    "# Import preprocessing    \n",
    "pos_tweets=[]\n",
    "neg_tweets=[]\n",
    "for tweet in tweets:\n",
    "    if tweet.startswith(\"1\\t\"):\n",
    "        pos_tweets.append(tweet.strip().replace(\"1\\t\",''))\n",
    "    elif tweet.startswith('0\\t'):\n",
    "        neg_tweets.append(tweet.strip().replace('0\\t',''))\n",
    "    else:\n",
    "        print('Input error: ', tweet)\n",
    "\n",
    "# Label vector\n",
    "y = np.concatenate((np.ones(len(pos_tweets)), np.zeros(len(neg_tweets))))\n",
    "\n",
    "# Test/train split (80/20)\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.concatenate((pos_tweets, neg_tweets)), y, test_size=0.2)\n",
    "\n",
    "# Ensure all is lowercase and remove any newlines (\\n)\n",
    "x_train = cleanText(x_train)\n",
    "x_test = cleanText(x_test)\n",
    "n_dim = 300\n",
    "\n",
    "# Build vocabulary\n",
    "welsh_w2v = Word2Vec(vector_size=n_dim, min_count=10)\n",
    "welsh_w2v.build_vocab(x_train)\n",
    "\n",
    "# Train the model\n",
    "welsh_w2v.train(x_train, total_examples=welsh_w2v.corpus_count, epochs=20)\n",
    "train_vecs = np.concatenate([buildWordVector(z, n_dim) for z in x_train])\n",
    "train_vecs = scale(train_vecs)\n",
    "\n",
    "# word2vec with test tweets\n",
    "welsh_w2v.train(x_test, total_examples=welsh_w2v.corpus_count, epochs=20)\n",
    "\n",
    "# Build test tweet vectors then scale\n",
    "test_vecs = np.concatenate([buildWordVector(z, n_dim) for z in x_test])\n",
    "test_vecs = scale(test_vecs)\n",
    "\n",
    "lr = SGDClassifier(loss='log', penalty='l1')\n",
    "lr.fit(train_vecs, y_train)\n",
    "\n",
    "print('Test Accuracy: %.2f'%lr.score(test_vecs, y_test))\n",
    "# 68%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
