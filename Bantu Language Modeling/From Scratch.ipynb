{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\SLU\\AI MSc\\Fall 22\\NLP\\Project 4\\sw-train.txt\", 'r') as f:\n",
    "    line = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractnig tokens from file\n",
    "# output: a list of all words, a list of all unique words\n",
    "\n",
    "def get_tokens(line):\n",
    "    tokens = line[0].split()\n",
    "    unique_tokens = set(tokens)\n",
    "    return tokens, unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, unique_tokens = get_tokens(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yaweze'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yaweze\n",
      "['y', 'a', 'w', 'e', 'z', 'e']\n"
     ]
    }
   ],
   "source": [
    "x = tokens[5]\n",
    "print(x)\n",
    "print([*x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kuishi',\n",
       " 'vema',\n",
       " 'maisha',\n",
       " 'ya',\n",
       " 'kikristo,',\n",
       " 'yaweze',\n",
       " 'kutoa',\n",
       " 'mchango',\n",
       " 'wao',\n",
       " 'kwa']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokens[:1000]\n",
    "x[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kuish'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(z) < 3:\n",
    "    n = 2\n",
    "elif len(z) == 3:\n",
    "    n = 3\n",
    "elif len(z) == 4:\n",
    "    n = 4\n",
    "else:\n",
    "    n = 5\n",
    "print('order of n-gram is:', n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "mwak\n"
     ]
    }
   ],
   "source": [
    "token = 'mwaka'\n",
    "print(token[-1])\n",
    "print(token[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = ' !\"\\'(),-.0123456789:;?abcdefghijklmnopqrstuvwxyz'\n",
    "vocabulary_list = [*vocabulary]\n",
    "token = 'warryn'\n",
    "token_prob_dic = {}\n",
    "gram_degree = len(token)\n",
    "for k in range(len(vocabulary_list)):\n",
    "    ch = vocabulary_list[k]\n",
    "    token = 'warry' + ch\n",
    "    gram_dic = {}\n",
    "    for j in range(len(token)):\n",
    "        c_token = 0\n",
    "        c_history = 0\n",
    "        token_list = []\n",
    "        history_list = []\n",
    "        for i in range(len(train)):\n",
    "            if token in train[i]:\n",
    "                c_token += 1\n",
    "                token_list.append(train[i])\n",
    "            else:\n",
    "                pass        # backoff\n",
    "            if token[:-1] in train[i]:\n",
    "                c_history += 1\n",
    "                history_list.append(train[i])\n",
    "        if len(token_list) != 0:\n",
    "            gram_dic[gram_degree] = c_token/c_history\n",
    "        if len(token) > 1:\n",
    "            token = token[1:]\n",
    "            gram_degree = len(token)\n",
    "        else:\n",
    "            pass\n",
    "    token_prob_dic[ch] = gram_dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m': {3: 0.003194888178913738,\n",
       "  2: 0.0009795347784069188,\n",
       "  1: 0.24069805832154748},\n",
       " 'n': {4: 0.016194331983805668,\n",
       "  3: 0.009584664536741214,\n",
       "  2: 0.0009352342105392691,\n",
       "  1: 0.32741098576080263},\n",
       " 'o': {3: 0.01597444089456869, 2: 0.16664232870036902, 1: 0.1890770646961082}}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_prob_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name(word):\n",
    "    chars = 'mno'\n",
    "    chars_list = [*chars]\n",
    "    token_prob_dic = {}\n",
    "    gram_degree = len(word)\n",
    "    for k in range(len(chars_list)):\n",
    "        ch = chars_list[k]\n",
    "        token = word[:-1] + ch\n",
    "        gram_dic = {}\n",
    "        for j in range(len(token)):\n",
    "            c_token = 0\n",
    "            c_history = 0\n",
    "            token_list = []\n",
    "            history_list = []\n",
    "            for i in range(len(train)):\n",
    "                if token in train[i]:\n",
    "                    c_token += 1\n",
    "                    token_list.append(train[i])\n",
    "                else:\n",
    "                    pass        # backoff\n",
    "                if token[:-1] in train[i]:\n",
    "                    c_history += 1\n",
    "                    history_list.append(train[i])\n",
    "            if len(token_list) != 0:\n",
    "                gram_dic[gram_degree] = c_token/c_history\n",
    "            if len(token) > 1:\n",
    "                token = token[1:]\n",
    "                gram_degree = len(token)\n",
    "            else:\n",
    "                pass\n",
    "        token_prob_dic[ch] = gram_dic\n",
    "    return token_prob_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m': {3: 0.003194888178913738,\n",
       "  2: 0.0009795347784069188,\n",
       "  1: 0.24069805832154748},\n",
       " 'n': {4: 0.016194331983805668,\n",
       "  3: 0.009584664536741214,\n",
       "  2: 0.0009352342105392691,\n",
       "  1: 0.32741098576080263},\n",
       " 'o': {3: 0.01597444089456869, 2: 0.16664232870036902, 1: 0.1890770646961082}}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = name('warryn')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(probs_dic):\n",
    "    chars_probs = {}\n",
    "    for i in probs_dic.keys():\n",
    "        char_prob = (1/len(probs_dic[i]))*sum(list(probs_dic[i].values()))\n",
    "        chars_probs[i] = char_prob\n",
    "    return chars_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(chars_probs):\n",
    "    probs_sum = sum(list(chars_probs.values()))\n",
    "    return {key: value / probs_sum for key, value in chars_probs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = -(1/len(x))*sum(np.log2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, devset, and test data\n",
    "def get_data(tokens, train_rate):\n",
    "    np.random.shuffle(tokens)\n",
    "    train_size = round(len(tokens) * train_rate)\n",
    "    test_devset_size = len(tokens) - train_size\n",
    "    train = tokens[:train_size]\n",
    "    devset = tokens[train_size:round(train_size + test_devset_size/2)]\n",
    "    test = tokens[round(train_size + test_devset_size/2):]\n",
    "    return train, devset, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, devset, test = get_data(tokens, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grams dictionary\n",
    "# output: a dictionary with number of charachters as keys and a list of same lengths tokens as values\n",
    "\n",
    "def get_gram(tokens):\n",
    "    longest_gram = max(tokens, key=len)\n",
    "    grams = {}\n",
    "    n = len(longest_gram)\n",
    "    while n > 0:\n",
    "        list_of_words = []\n",
    "        for i in range(len(tokens)):\n",
    "            if len(tokens[i]) == n:\n",
    "                list_of_words.append(tokens[i])\n",
    "        if len(list_of_words) > 0:\n",
    "            grams[n] = list_of_words\n",
    "        n -= 1\n",
    "    return grams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9886d89d00ab45e82c8ee75015344ff9f539db9369165debfcbb1d4785acc62d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
